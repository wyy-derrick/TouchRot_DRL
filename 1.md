这是一份为您重新整理的、针对程序员开发的**详细代码构建说明书**。

这份文档修正了传感器数量（17个），明确了**SAC作为核心算法**的构建细节，规划了PPO/TD3的接口，并严格区分了“训练（无头模式+TensorBoard）”与“演示（可视化）”的逻辑。

---

# 项目构建指南：基于MuJoCo与SAC的灵巧手手内旋转任务

## 1. 项目概述 (Project Overview)

* **目标**：使用强化学习（RL）控制Leap Hand灵巧手，利用关节感知和**17个触觉传感器**的二值化信息，完成立方体绕固定轴（如Z轴）的定向旋转任务。
* **核心算法**：SAC (Soft Actor-Critic) [当前优先级: High]。
* **扩展算法**：PPO, TD3 [预留接口，后续构建]。
* **训练模式**：静默训练（无渲染），使用 TensorBoard 记录曲线，保存 `.pth` 模型权重。
* **演示模式**：加载训练好的权重，开启 MuJoCo 渲染窗口进行效果展示。

---

## 2. 代码文件结构 (File Structure)

程序员需严格按照以下结构创建文件：

```text
LeapHand_RL/
│
├── assets/                          # [资源文件]
│   ├── scene_left(cubic).xml        # [User Provided] 场景入口
│   ├── left_hand(sensor).xml        # [User Provided] 包含17个传感器的手模型
│   └── *.obj, *.stl                 # 网格文件
│
├── envs/                            # [环境定义]
│   ├── __init__.py
│   └── leap_hand_env.py             # 核心环境类 (Gym API风格)
│
├── utils/                           # [工具模块]
│   ├── tactile_utils.py             # [User Logic] 封装触觉检测与二值化逻辑
│   ├── math_utils.py                # 旋转计算、四元数转欧拉角等
│   └── logger.py                    # TensorBoard 封装
│
├── algorithms/                      # [算法实现]
│   ├── __init__.py
│   ├── base_agent.py                # 算法基类 (定义统一接口)
│   ├── sac.py                       # [Core] SAC Agent 逻辑 (update, save, load)
│   ├── sac_models.py                # [Core] SAC 网络架构 (Actor, Critic)
│   ├── replay_buffer.py             # 经验回放池
│   ├── ppo.py                       # [TODO] 预留文件
│   └── td3.py                       # [TODO] 预留文件
│
├── checkpoints/                     # [输出] 存放 .pth 模型文件
│   └── sac/
├── logs/                            # [输出] 存放 TensorBoard 日志
│
├── configs/                         # [配置]
│   └── config.yaml                  # 所有超参数 (学习率, 奖励权重, 传感器阈值等)
│
├── train.py                         # [入口] 训练主脚本 (无渲染)
└── demo.py                          # [入口] 演示主脚本 (有渲染)

```

---

## 3. 详细模块实现说明

### 3.1 工具模块: `utils/tactile_utils.py`

**功能**：处理您提供的 XML 中定义的传感器数据。
**需实现内容**：

* **传感器列表定义**：必须包含以下 **17** 个名称（已核对 XML）：
```python
SENSOR_NAMES = [
    "palm_1_sensor", "palm_2_sensor", "palm_3_sensor", "palm_7_sensor", "palm_8_sensor", # 手掌(5)
    "if_px_sensor", "mf_px_sensor", "rf_px_sensor", "th_px_sensor",                      # 近节(4)
    "if_md_sensor", "mf_md_sensor", "rf_md_sensor", "th_ds_sensor",                      # 中节/拇指远节(4)
    "if_tip_sensor", "mf_tip_sensor", "rf_tip_sensor", "th_tip_sensor"                   # 指尖(4)
]

```


* **逻辑迁移**：将 `env1_tolerance.py` 中的 `get_sensor_data_with_tolerance` 函数移植过来。
* **二值化接口**：提供 `get_binary_tactile_state(data, threshold=0.01)` 函数，返回 shape 为 `(17,)` 的 numpy 数组（0或1）。

### 3.2 环境模块: `envs/leap_hand_env.py`

**功能**：标准的 RL 环境封装。
**需实现内容**：

1. **初始化 (`__init__`)**:
* 加载 `scene_left(cubic).xml`。
* 定义动作空间：`Box(-1, 1, shape=(16,))` (16个关节)。
* 定义观测空间：
* `qpos` (16 dim) + `qvel` (16 dim)
* `tactile` (17 dim, binary)
* `target_info` (如目标旋转轴 vector, 3 dim)
* **Total Dim = 52** (若堆叠历史帧则更多)。




2. **重置 (`reset`)**:
* **物体初始化**：调用 `reset_box_position`（参考您的py代码）。
* **方向固定**：因为是定向旋转，物体初始姿态的四元数可以添加少量随机噪声，但整体朝向固定，确保旋转轴（如 Z 轴）相对于手掌是明确的。


3. **步进 (`step`)**:
* **动作执行**：实现相对位置控制。
`ctrl = current_qpos + action * scaling_factor`。
* **物理仿真**：`mujoco.mj_step(model, data)`。
* **奖励计算 ()**：严格复现论文公式。
* : 计算物体当前旋转角与上一帧旋转角的增量（绕 Z 轴）。
* : 指尖位置与物体表面的距离惩罚（鼓励接触）。
* : 惩罚物体线速度（防止飞出）。
* : 惩罚动作幅度与力矩。
* *公式参考*: 。


* **终止条件 (`done`)**:
* 物体掉落（Z坐标低于阈值）。
* 达到最大步数。





### 3.3 核心算法: SAC构建 (`algorithms/sac*.py`)

这是您要求**最详细**的部分，程序员需要照此编写。

#### A. `algorithms/base_agent.py` (接口定义)

```python
class BaseAgent:
    def select_action(self, state, evaluate=False):
        raise NotImplementedError
    def update(self, replay_buffer, batch_size):
        raise NotImplementedError
    def save(self, path):
        raise NotImplementedError
    def load(self, path):
        raise NotImplementedError

```

* *目的*：后续写 PPO 和 TD3 时，直接继承此类，保持 `train.py` 代码不变。

#### B. `algorithms/sac_models.py` (网络架构)

**1. Class `Actor` (策略网络)**

* **输入**: `state_dim` (e.g., 52)。
* **结构**: MLP (e.g., 256 -> 256 -> 256)。
* **输出**: `mean` (均值) 和 `log_std` (对数标准差)，维度均为 `action_dim` (16)。
* **关键逻辑**:
* 使用 `reparameterization trick` (重参数化技巧) 采样动作。
* 应用 `tanh` 将动作压缩到 [-1, 1]。
* **必须实现** `sample(state)` 方法，返回 `(action, log_prob, mean)`。



**2. Class `Critic` (价值网络)**

* **输入**: `state_dim + action_dim`。
* **结构**: Double Q-Network 结构 (两个独立的 MLP)。
* `Q1`: Input -> [256, 256] -> Output(1)
* `Q2`: Input -> [256, 256] -> Output(1)


* **目的**: 缓解 Q 值过高估计 (Overestimation Bias)。

#### C. `algorithms/sac.py` (逻辑实现)

**Class `SACAgent(BaseAgent)**`:

* **属性**: 包含 `actor`, `critic`, `critic_target` 以及对应的 Optimizers。还有自动调整熵的系数 `alpha`。
* **方法 `update(batch)**`:
1. **Critic Update**:
* 从 buffer 采样 。
* 计算目标 Q 值：。
* 计算当前 Q 值：。
* Loss: MSE(Q1, y) + MSE(Q2, y)。
* `critic_optimizer.step()`。


2. **Actor Update**:
* 重新根据当前策略采样: 。
* Loss: 。
* `actor_optimizer.step()`。


3. **Alpha Update (可选)**: 自动调整熵正则化项。
4. **Soft Update**: 使用 `tau` 系数平滑更新 Target Critic 参数。



---

## 4. 训练与可视化流程

### 4.1 训练脚本: `train.py`

**注意**：此脚本**不涉及** `mujoco.viewer`，确保服务器或后台可运行。

1. **设置**:
* 读取 `config.yaml`。
* 实例化 `LeapHandEnv`。
* 根据配置实例化 `SACAgent` (未来可用 if-else 切换 PPO/TD3)。
* 实例化 `ReplayBuffer`。
* 实例化 `SummaryWriter` (TensorBoard)。


2. **循环**:
* `env.reset()`。
* `while not done`:
* `agent.select_action(state)`。
* `env.step(action)` -> 得到 `next_state, reward, done`。
* `buffer.push(...)`。
* **如果 buffer 满了**: 执行 `agent.update()`。
* **日志记录**: 每 N步 记录一次 `loss`, `reward` 到 TensorBoard。


* **保存**: 每 M 个 Episode 保存一次 `model_ep_{}.pth` 到 `checkpoints/` 目录。



### 4.2 演示脚本: `demo.py`

**注意**：此脚本**必须**包含可视化。

1. **设置**:
* 创建环境。
* **关键**: 启动 `mujoco.viewer.launch_passive(model, data)`。
* 实例化 `SACAgent` 并调用 `agent.load('checkpoints/best_model.pth')`。


2. **循环**:
* `state = env.reset()`
* `while True`:
* `action = agent.select_action(state, evaluate=True)` (确定性策略，不加噪声)。
* `env.step(action)`。
* **可视化同步**: `viewer.sync()`。
* `time.sleep(dt)` 控制帧率以便肉眼观察。


* 在终端打印当前的触觉传感器状态（哪些被激活），便于您检查效果。



---

## 5. 程序员开发Checklist

* [ ] **XML核对**: 确认 `left_hand(sensor).xml` 中的 `<sensor>` 标签确实是 17 个，且名称与 `utils/tactile_utils.py` 中的列表一致。
* [ ] **TensorBoard**: 确保 Loss 曲线（Actor Loss, Critic Loss）和 Reward 曲线（Episode Reward）都能正常绘制。
* [ ] **无头模式**: 确保 `train.py` 运行时没有弹窗，且 GPU 占用正常。
* [ ] **模型保存**: 训练结束后，检查 `checkpoints/` 文件夹下是否有 `.pth` 文件。
* [ ] **PPO/TD3接口**: 检查 `BaseAgent` 是否足够通用，方便后续直接添加新算法文件。